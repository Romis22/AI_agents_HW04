# Sportka RL - AI Agent pro optimalizaci loterie

Tento projekt implementuje Deep Q-Learning (DQN) agenta pro optimalizaci strategie hranÃ­ ÄeskÃ© loterie **Sportka**. Agent se uÄÃ­ na zÃ¡kladÄ› historickÃ½ch dat a snaÅ¾Ã­ se minimalizovat ztrÃ¡ty pÅ™i hranÃ­.

## ğŸ¯ PÅ™ehled

Program obsahuje:
- **DQN agenta** natrÃ©novanÃ©ho pomocÃ­ Deep Reinforcement Learning
- **SimulaÄnÃ­ prostÅ™edÃ­** pro Sportku s realistickÃ½mi pravidly
- **NÃ¡hodnÃ©ho agenta** pro porovnÃ¡nÃ­ vÃ½konu
- **KompletnÃ­ testovÃ¡nÃ­ a statistickÃ© vyhodnocenÃ­** obou pÅ™Ã­stupÅ¯
- **Vizualizace vÃ½sledkÅ¯** pomocÃ­ grafÅ¯

## ğŸ“ Struktura projektu

```
v4/
â”œâ”€â”€ main.py          # TestovÃ¡nÃ­ a porovnÃ¡nÃ­ agentÅ¯
â”œâ”€â”€ train.py         # TrÃ©novÃ¡nÃ­ DQN agenta
â”œâ”€â”€ models.py        # Definice DQN a nÃ¡hodnÃ©ho agenta
â”œâ”€â”€ environment.py   # SimulaÄnÃ­ prostÅ™edÃ­ pro Sportku
â””â”€â”€ README.md        # Tento soubor
```

### Popis souborÅ¯:

- **`environment.py`** - Gymnasium prostÅ™edÃ­ simulujÃ­cÃ­ Sportku s historickÃ½mi daty
- **`models.py`** - DQN neural network a RandomAgent pro porovnÃ¡nÃ­
- **`train.py`** - TrÃ©novÃ¡nÃ­ agenta (2000 epizod)
- **`main.py`** - TestovÃ¡nÃ­ a porovnÃ¡nÃ­ vÃ½konu agentÅ¯

## ğŸš€ Instalace a spuÅ¡tÄ›nÃ­

### PoÅ¾adavky:
```bash
pip install torch gymnasium numpy matplotlib scipy
```

### PouÅ¾itÃ­:

1. **TrÃ©novÃ¡nÃ­ agenta:**
```bash
python train.py
```

2. **TestovÃ¡nÃ­ a porovnÃ¡nÃ­:**
```bash
python main.py
```

## ğŸ§  Algoritmus

### DQN Agent:
- **Architektura**: 3-vrstvÃ¡ neuronovÃ¡ sÃ­Å¥ s dropout (512â†’512â†’256â†’50 neuronÅ¯)
- **Vstup**: Historie poslednÃ­ch 5 losovÃ¡nÃ­ + aktuÃ¡lnÃ­ balance
- **VÃ½stup**: PravdÄ›podobnosti pro 49 ÄÃ­sel + Å ance
- **Strategie**: Epsilon-greedy s postupnÃ½m sniÅ¾ovÃ¡nÃ­m explorace

### Reward systÃ©m:
- **ZÃ¡kladnÃ­ reward**: `(vÃ½hry - nÃ¡klady) / 100`
- **Bonus za vÃ½hru**: `poÄet_shod Ã— 10` bodÅ¯
- **Penalizace**: `-2 Ã— poÄet_opakujÃ­cÃ­ch_se_ÄÃ­sel` (pÅ™i >2 opakovÃ¡nÃ­ch)

## ğŸ² VÃ½hernÃ­ tabulka

| Shody | VÃ½hra | PravdÄ›podobnost |
|-------|-------|-----------------|
| 3     | 50 KÄ | ~1:57 |
| 4     | 300 KÄ | ~1:1,033 |
| 5     | 10,000 KÄ | ~1:55,492 |
| 6     | 100,000 KÄ | ~1:13,983,816 |

### SpeciÃ¡lnÃ­ bonusy:
- **Å ance**: +1,000 KÄ (pÅ™i shodÄ› poslednÃ­ho ÄÃ­sla)
- **Superjackpot**: +1,000,000 KÄ (6 shod + Å ance)

**NÃ¡klady**: 30 KÄ za sloupeÄek + 30 KÄ za Å anci

## ğŸ“Š VÃ½sledky

Program produkuje:

### BÄ›hem trÃ©novÃ¡nÃ­:
- `training_results.png` - Grafy prÅ¯bÄ›hu uÄenÃ­
- `sportka_model.pth` - UloÅ¾enÃ½ natrÃ©novanÃ½ model

### PÅ™i testovÃ¡nÃ­:
- `comparison_results.png` - PorovnÃ¡nÃ­ DQN vs Random agenta
- DetailnÃ­ statistiky v konzoli:
  - PrÅ¯mÄ›rnÃ½ ROI, zÅ¯statky, vÃ½hry
  - PoÄty vÃ½her podle kategoriÃ­
  - StatistickÃ¡ vÃ½znamnost (t-test)

### PÅ™Ã­klad vÃ½stupu:
```
DQN Agent:
  PrÅ¯mÄ›rnÃ½ koneÄnÃ½ zÅ¯statek: 12,450.00 KÄ
  PrÅ¯mÄ›rnÃ© vÃ½hry: 8,230.00 KÄ
  PrÅ¯mÄ›rnÃ© ROI: -58.5%

Random Agent:
  PrÅ¯mÄ›rnÃ½ koneÄnÃ½ zÅ¯statek: 11,890.00 KÄ
  PrÅ¯mÄ›rnÃ© vÃ½hry: 7,960.00 KÄ
  PrÅ¯mÄ›rnÃ© ROI: -60.3%

RelativnÃ­ zlepÅ¡enÃ­: +3.1%
```

## âš™ï¸ TechnickÃ© detaily

### Hyperparametry:
- **Epizody**: 2,000
- **Learning rate**: 0.001
- **Batch size**: 32
- **Epsilon decay**: 0.995 (1.0 â†’ 0.01)
- **Target network update**: kaÅ¾dÃ½ch 100 epizod

### ProstÅ™edÃ­:
- **PoÄÃ¡teÄnÃ­ balance**: 30,000 KÄ
- **MaximÃ¡lnÃ­ tikety**: 1,000 per epizoda
- **Observation space**: 5Ã—49 (historie) + 2 (balance, counter)
- **Action space**: 50 (49 ÄÃ­sel + Å ance)

## ğŸ“ˆ OÄekÃ¡vanÃ© vÃ½sledky

âš ï¸ **DÅ¯leÅ¾itÃ©**: Oba pÅ™Ã­stupy jsou v dlouhodobÃ©m prÅ¯mÄ›ru **ztrÃ¡tovÃ©**, coÅ¾ odpovÃ­dÃ¡ realitÄ› loteriÃ­. DQN agent se snaÅ¾Ã­ **minimalizovat ztrÃ¡ty**, nikoliv vyhrÃ¡vat.

TypickÃ¡ zlepÅ¡enÃ­ DQN agenta:
- **ROI**: +2-5% relativnÃ­ zlepÅ¡enÃ­ oproti nÃ¡hodnÃ©mu sÃ¡zenÃ­
- **Variance**: NiÅ¾Å¡Ã­ volatilita vÃ½sledkÅ¯
- **Strategie**: VyhÃ½bÃ¡nÃ­ se opakovÃ¡nÃ­ nedÃ¡vnÃ½ch ÄÃ­sel

## ğŸ”§ MoÅ¾nÃ¡ rozÅ¡Ã­Å™enÃ­

- **ReÃ¡lnÃ¡ historickÃ¡ data** mÃ­sto simulovanÃ½ch losovÃ¡nÃ­
- **RÅ¯znÃ© strategie sÃ¡zenÃ­** (progresivnÃ­, proporcionÃ¡lnÃ­)
- **Ensemble metody** (kombinace vÃ­ce modelÅ¯)
- **AnalÃ½za hot/cold ÄÃ­sel** s delÅ¡Ã­ historiÃ­
- **Optimalizace pro rÅ¯znÃ© rozpoÄty**

## ğŸ“ PoznÃ¡mky

- Program pouÅ¾Ã­vÃ¡ **zjednoduÅ¡enou vÃ½hernÃ­ tabulku**
- Simulace **neobsahuje progresivnÃ­ jackpoty**
- Å ance se poÄÃ­tÃ¡ jako `(drawn_chance == sum(player_numbers) % 10)`
- Model je urÄen **pouze pro vÃ½zkumnÃ© ÃºÄely**

## ğŸ“ ZÃ¡vÄ›r

Tento projekt demonstruje aplikaci Deep Reinforcement Learning na reÃ¡lnÃ½ problÃ©m optimalizace loterie. I kdyÅ¾ nenÃ­ moÅ¾nÃ© "porazit" loterii, DQN agent dokÃ¡Å¾e nalÃ©zt strategie, kterÃ© mÃ­rnÄ› sniÅ¾ujÃ­ oÄekÃ¡vanÃ© ztrÃ¡ty oproti nÃ¡hodnÃ©mu sÃ¡zenÃ­.

---

*âš ï¸ UpozornÄ›nÃ­: Tento program je urÄen pouze pro vzdÄ›lÃ¡vacÃ­ a vÃ½zkumnÃ© ÃºÄely. Hazard mÅ¯Å¾e bÃ½t nÃ¡vykovÃ½.*